# AI模型训练过程

大型语言模型(LLM)的训练流程通常分为三个关键阶段：pre-training -> post-training -> RL

## 一、预训练(Pretraining)

![[pre-training.png]]

### Step 1：数据预处理

数据预处理的核心目标是获取**高质量、干净、去重且隐私安全**的数据，作为语言模型的训练基础。

**具体步骤：**

1. **URL过滤**：过滤不可信URL，确保数据来源安全可靠
2. **文本提取**：从网页提取纯文本，去除HTML标记等无关信息
3. **语言过滤**：筛选目标语言内容，保持语言一致性
4. **Gopher过滤**：进一步筛选，去除低质量内容
5. **MinHash去重**：检测和去除重复数据，确保数据集多样性
6. **C4过滤器**：应用C4数据集规则清理数据，去除噪声
7. **自定义过滤器**：根据特定需求应用额外处理
8. **PII移除**：删除个人身份信息保护隐私

这些步骤组成了FineWeb pipeline，确保获得适合训练的高质量数据集。

### Step 2：标记化(Tokenization)

标记化将清洗后的文本转换为模型可处理的**tokens(标记)**，即符号化的最小单元。

标记化过程将连续文本切分为离散单元，例如：

- 使用BPE(Byte-Pair Encoding)算法递归合并常见token对
- 创建从无限语言空间到有限计算空间的映射
- 最终形成约50,000个token的词汇表

### Step 3：神经网络训练

神经网络训练阶段，模型接收token序列输入，学习预测下一个可能出现的token。模型通过不断调整参数，学习如何根据上下文生成合理的概率分布。

训练过程展示了模型如何处理输入序列并学习预测下一个token的概率分布。神经网络内部使用大量参数("weights")通过矩阵计算生成这些预测，现代大模型通常拥有数十亿至数千亿参数。

### Step 4：推理(Inference)

训练完成后，模型在实际应用中：

- 接收已有token序列
- 计算所有可能token的概率分布
- 从分布中采样选择下一个token
- 将选择的token添加到序列中，然后重复这个过程

这种自回归生成方式使模型能够一个接一个地生成token，每次都依赖于之前生成的所有内容。

预训练阶段最终产出的是一个"互联网文档模拟器"，这反映了基础模型本质上是从互联网文本中学习生成类似内容的能力，但尚未优化为能与人类对话的助手。

## 二、后训练（post-training）

![[post-training.png]]

后训练是将预训练的基础模型转变为能够回答人类问题的对话助手的阶段，其主要方法是监督微调(SFT)。

### 数据准备与训练过程

SFT使用人工标注的对话数据集，包含：

- 人类提问和助手回答的对话格式
- 不同类型的对话示例(数学、科学、日常问答等)
- 助手回应的统一风格(有礼貌、提供帮助、拒绝不适当请求)

专业标注员首先设计提示词，随后编写符合要求的理想助手回复。对话数据集的发展经历了从纯人工标注到LLM辅助标注的演进，现代数据集也包含完全合成的对话。

从技术角度看，SFT与预训练使用相似的训练方法(如交叉熵损失函数)，但针对的是对话数据而非普通文本，这使模型学会了如何以助手的身份回应人类问题。

如一个简单的概念区分：SFT是"模仿我做的事"，而后续的RLHF是"学习我喜欢什么"。

### 幻觉问题与解决策略

后训练阶段也关注解决模型的幻觉问题。幻觉的根本原因是模型本质是通过预测token运行，并不真正理解知识，只是从概率分布中采样。

减轻幻觉的两种主要策略：

1. **教导模型承认无知**：训练模型在不知道答案时回答"I don't know"
    - 例："Who is Orson Kovacs?" - "I'm sorry, I don't believe I know"

2. **赋予模型搜索能力**：训练模型使用特定格式调用搜索工具
    - 例：`<SEARCH_START>Who is Orson Kovacs?<SEARCH_END>`
    - 搜索后模型可以给出基于事实的回答

除搜索外，模型还可以使用网络搜索、代码解释器等工具增强回答能力，这些能力在SFT阶段被有意识地训练。

## 三、基于强化学习的优化(RL)

![[RL.png]]

强化学习是构建对话模型的最后阶段，建立在SFT模型基础上，采用全新的训练范式来进一步优化模型表现。

### 强化学习的工作原理

![[知识课本解读模型训练.png]]

强化学习可以类比为教育过程中的练习题环节：

- **预训练**：相当于阅读教材，构建基础知识
- **SFT**：相当于学习示例解答，掌握基本解题方法
- **RL**：相当于做练习题，通过实践尝试不同解法，强化正确方法

强化学习的具体过程：

1. 给定问题(prompt)和期望答案
2. 让SFT模型尝试多种解决方案
3. 评估这些解决方案，选择正确且高效的方案
4. 在这些优质方案上进行训练，增加它们的生成概率
5. 反复多次迭代，将最佳实践内化到模型中

例如数学问题场景：

- 问题："Emily buys 3 apples and 2 oranges. Each orange costs $2. The total cost of all the fruit is $13. What is the cost of each apple?"
- 模型生成多种解法(图中显示15种)
- 仅有部分解法得到正确答案(每个苹果$3)
- 强化学习会优先选择这些正确且简洁的解法

### 强化学习的核心原理

强化学习本质是一种试错机制：

- 探索多种可能的解决方案
- 对这些方案进行客观评估
- 强化有效方案的采用概率，降低低效方案的概率
- 通过大量迭代，模型逐渐掌握最佳解题策略

这种方法比单纯的监督学习更有效，因为它不仅学习"如何回答"，还学习"如何更好地回答"，优化了模型的整体表现。

## 总结

大型语言模型的训练流程包含三个相互关联的阶段：

1. **预训练(Pretraining)**：通过对互联网文档的大规模训练，构建基础语言能力，产出的是一个"互联网文档模拟器"。
    
2. **后训练(Post-training)**：主要通过监督微调(SFT)，使用人类标注的对话数据集，将基础模型转变为初步的对话助手，教会模型如何按照人类期望的方式回答问题。
    
3. **强化学习(RL)**：在SFT模型基础上，通过让模型尝试多种解法并强化最佳方案，进一步优化模型的回答质量，使其提供更准确、更有用的回应。
    

这三个阶段构成了从基础能力到专业助手的完整进化路径，形成了现代大型语言模型训练的标准流程。每个阶段都针对特定目标进行优化，共同打造出类似ChatGPT这样能够有效对话并解决问题的AI助手。