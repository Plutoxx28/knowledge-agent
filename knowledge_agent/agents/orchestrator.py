"""
ä¸»ç¼–æ’Agent - åè°ƒå„ä¸ªå·¥ä½œè€…Agentçš„æ‰§è¡Œ
"""
import uuid
import asyncio
from typing import Dict, List, Any, Optional, Callable
from agents.base_agent import BaseAgent
from agents.content_parser import ContentParser
from agents.structure_builder import StructureBuilder
from agents.link_discoverer import LinkDiscoverer
from utils.vector_db import LocalVectorDB
from utils.text_processor import TextProcessor
from utils.link_manager import LinkManager
from utils.progress_websocket import create_progress_callback, ProgressBroadcaster
import logging
import os
from enum import Enum
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)

class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦ç­‰çº§"""
    SIMPLE = "simple_task"
    MEDIUM = "medium_task" 
    COMPLEX = "complex_task"

class ProcessingStage(Enum):
    """å¤„ç†é˜¶æ®µ"""
    ANALYZING = "analyzing"
    GENERATING_WORKERS = "generating_workers"
    WORKER_PROCESSING = "worker_processing"
    FINALIZING = "finalizing"
    COMPLETED = "completed"

@dataclass
class ProcessingProgress:
    """å¤„ç†è¿›åº¦ä¿¡æ¯"""
    task_id: str
    complexity: TaskComplexity
    stage: ProcessingStage
    current_step: str
    total_steps: int
    completed_steps: int
    workers: List[str] = None
    error: str = None
    start_time: float = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "complexity": self.complexity.value,
            "stage": self.stage.value,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "completed_steps": self.completed_steps,
            "progress_percent": (self.completed_steps / self.total_steps * 100) if self.total_steps > 0 else 0,
            "workers": self.workers or [],
            "error": self.error,
            "elapsed_time": time.time() - self.start_time if self.start_time else 0
        }

class KnowledgeOrchestrator(BaseAgent):
    """çŸ¥è¯†æ•´ç†ä¸»ç¼–æ’Agent"""
    
    def __init__(self, knowledge_base_path: str, vector_db_path: str = "./data/chroma_db", 
                 progress_callback: Optional[Callable[[ProcessingProgress], None]] = None,
                 enable_websocket: bool = True):
        super().__init__(
            name="çŸ¥è¯†æ•´ç†ç¼–æ’ä¸“å®¶",
            description="åè°ƒå„ä¸ªå·¥ä½œè€…Agentå®ŒæˆçŸ¥è¯†æ•´ç†ä»»åŠ¡"
        )
        
        self.knowledge_base_path = knowledge_base_path
        self.current_progress: Optional[ProcessingProgress] = None
        
        # è®¾ç½®è¿›åº¦å›è°ƒ
        if progress_callback is None and enable_websocket:
            # ä½¿ç”¨é»˜è®¤çš„WebSocketå¹¿æ’­å™¨
            self.progress_callback = create_progress_callback()
        else:
            self.progress_callback = progress_callback
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_db = LocalVectorDB(vector_db_path)
        
        # åˆå§‹åŒ–é“¾æ¥ç®¡ç†å™¨
        self.link_manager = LinkManager(knowledge_base_path)
        
        # åˆå§‹åŒ–å·¥ä½œè€…Agents
        self.content_parser = ContentParser()
        self.structure_builder = StructureBuilder()
        self.link_discoverer = LinkDiscoverer(self.vector_db)
        self.text_processor = TextProcessor()
        
        logger.info("çŸ¥è¯†æ•´ç†ç¼–æ’Agentåˆå§‹åŒ–å®Œæˆ")
    
    def _update_progress(self, stage: ProcessingStage, current_step: str, completed_steps: int = None, 
                        workers: List[str] = None, error: str = None):
        """æ›´æ–°å¤„ç†è¿›åº¦"""
        if self.current_progress:
            self.current_progress.stage = stage
            self.current_progress.current_step = current_step
            if completed_steps is not None:
                self.current_progress.completed_steps = completed_steps
            if workers is not None:
                self.current_progress.workers = workers
            if error is not None:
                self.current_progress.error = error
            
            logger.info(f"ğŸ”„ è¿›åº¦æ›´æ–°: {stage.value} - {current_step} (å›è°ƒå­˜åœ¨: {self.progress_callback is not None})")
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if self.progress_callback:
                try:
                    logger.info(f"ğŸ“¤ è°ƒç”¨è¿›åº¦å›è°ƒ: {self.current_progress.to_dict()}")
                    self.progress_callback(self.current_progress)
                    logger.info(f"âœ… è¿›åº¦å›è°ƒè°ƒç”¨æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ è¿›åº¦å›è°ƒè°ƒç”¨å¤±è´¥: {e}")
            else:
                logger.warning("âš ï¸ æ²¡æœ‰è¿›åº¦å›è°ƒå‡½æ•°")
                
            logger.info(f"è¿›åº¦æ›´æ–°: {stage.value} - {current_step}")
    
    def _determine_complexity(self, content: str, operation: str, options: Dict[str, Any]) -> TaskComplexity:
        """ç¡®å®šä»»åŠ¡å¤æ‚åº¦"""
        content_length = len(content)
        
        # å¤æ‚åº¦åˆ¤æ–­é€»è¾‘
        complexity_factors = {
            'length': content_length,
            'has_structure': bool(options.get('force_structure', False)),
            'requires_links': bool(options.get('enable_linking', True)),
            'batch_mode': bool(options.get('batch_mode', False)),
            'update_mode': operation == "update"
        }
        
        # ç®€å•ä»»åŠ¡ï¼šçŸ­æ–‡æœ¬ï¼Œæ— ç‰¹æ®Šè¦æ±‚
        if (content_length < 1000 and not complexity_factors['batch_mode'] 
            and not complexity_factors['update_mode']):
            return TaskComplexity.SIMPLE
        
        # å¤æ‚ä»»åŠ¡ï¼šé•¿æ–‡æœ¬æˆ–å¤šä¸ªå¤æ‚è¦æ±‚
        elif (content_length > 10000 or 
              sum([complexity_factors['has_structure'], 
                   complexity_factors['batch_mode'], 
                   complexity_factors['update_mode']]) >= 2):
            return TaskComplexity.COMPLEX
        
        # ä¸­ç­‰ä»»åŠ¡ï¼šå…¶ä»–æƒ…å†µ
        else:
            return TaskComplexity.MEDIUM
    
    def _get_worker_list(self, complexity: TaskComplexity, operation: str) -> List[str]:
        """æ ¹æ®å¤æ‚åº¦è·å–éœ€è¦çš„å·¥ä½œè€…åˆ—è¡¨"""
        base_workers = ["å†…å®¹è§£æå™¨", "ç»“æ„æ„å»ºå™¨"]
        
        if complexity == TaskComplexity.SIMPLE:
            # å•Agentç‹¬ç«‹å¤„ç†
            return ["å†…å®¹å¤„ç†Agent"]
        elif complexity == TaskComplexity.MEDIUM:
            # ç”Ÿæˆ3-4ä¸ªå·¥ä½œè€…
            workers = base_workers + ["é“¾æ¥å‘ç°å™¨"]
            if operation == "update":
                workers.append("å¢é‡æ›´æ–°å™¨")
            return workers
        else:  # COMPLEX
            # ç”Ÿæˆ5-6ä¸ªå·¥ä½œè€…å¹¶è¡Œå¤„ç†
            workers = base_workers + ["é“¾æ¥å‘ç°å™¨", "æ¦‚å¿µæå–å™¨", "å…³ç³»åˆ†æå™¨"]
            if operation == "update":
                workers.append("æ™ºèƒ½åˆå¹¶å™¨")
            return workers
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†çŸ¥è¯†æ•´ç†ä»»åŠ¡
        
        Args:
            input_data: {
                "content": str,          # è¾“å…¥å†…å®¹
                "type": str,             # å†…å®¹ç±»å‹ (auto/text/url/conversation/markdown)
                "metadata": dict,        # å…ƒæ•°æ®
                "operation": str,        # æ“ä½œç±»å‹ (create/update/analyze)
                "target_file": str,      # ç›®æ ‡æ–‡ä»¶ï¼ˆæ›´æ–°æ“ä½œæ—¶ä½¿ç”¨ï¼‰
                "options": dict          # å¤„ç†é€‰é¡¹
            }
        
        Returns:
            {
                "success": bool,
                "result": dict,          # å¤„ç†ç»“æœ
                "output_file": str,      # è¾“å‡ºæ–‡ä»¶è·¯å¾„
                "doc_id": str,          # æ–‡æ¡£ID
                "statistics": dict,      # å¤„ç†ç»Ÿè®¡
                "errors": List[str],     # é”™è¯¯ä¿¡æ¯
                "progress": dict         # æœ€ç»ˆè¿›åº¦ä¿¡æ¯
            }
        """
        
        content = input_data.get("content", "")
        content_type = input_data.get("type", "auto")
        metadata = input_data.get("metadata", {})
        operation = input_data.get("operation", "create")
        target_file = input_data.get("target_file", "")
        options = input_data.get("options", {})
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        result = {
            "success": False,
            "result": {},
            "output_file": "",
            "doc_id": "",
            "statistics": {},
            "errors": [],
            "task_id": task_id,
            "progress": {}
        }
        
        try:
            # 1. åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
            complexity = self._determine_complexity(content, operation, options)
            workers = self._get_worker_list(complexity, operation)
            
            # æ ¹æ®å¤æ‚åº¦è®¾ç½®æ€»æ­¥æ•°
            if complexity == TaskComplexity.SIMPLE:
                total_steps = 3  # åˆ†æ->å¤„ç†->å®Œæˆ
            elif complexity == TaskComplexity.MEDIUM:
                total_steps = 5  # åˆ†æ->ç”Ÿæˆå·¥ä½œè€…->å†…å®¹è§£æ->ç»“æ„æ„å»º->é“¾æ¥å‘ç°->å®Œæˆ
            else:  # COMPLEX
                total_steps = 7  # åˆ†æ->ç”Ÿæˆå·¥ä½œè€…->å¹¶è¡Œå¤„ç†(å¤šæ­¥)->å®Œæˆ
            
            self.current_progress = ProcessingProgress(
                task_id=task_id,
                complexity=complexity,
                stage=ProcessingStage.ANALYZING,
                current_step="Agentè¯†åˆ«ä¸­",
                total_steps=total_steps,
                completed_steps=0,
                workers=[],
                start_time=time.time()
            )
            
            # æ›´æ–°è¿›åº¦ï¼šåˆ†æé˜¶æ®µ
            self._update_progress(ProcessingStage.ANALYZING, "Agentè¯†åˆ«ä¸­", 0)
            
            # 2. åˆ†æä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©ç­–ç•¥  
            strategy = self._analyze_task_complexity(content, operation, options)
            logger.info(f"é€‰æ‹©å¤„ç†ç­–ç•¥: {strategy}")
            
            # 3. æ ¹æ®å¤æ‚åº¦æ˜¾ç¤ºä¸åŒçš„è¿›åº¦ä¿¡æ¯
            if complexity == TaskComplexity.SIMPLE:
                # ç®€å•ä»»åŠ¡ï¼šæ˜¾ç¤º "Agentå¤„ç†ä¸­"
                self._update_progress(ProcessingStage.WORKER_PROCESSING, "Agentå¤„ç†ä¸­", 1)
            else:
                # å¤æ‚ä»»åŠ¡ï¼šæ˜¾ç¤ºå·¥ä½œè€…ç”Ÿæˆ
                self._update_progress(ProcessingStage.GENERATING_WORKERS, 
                                    f"ç”Ÿæˆäº†{len(workers)}ä¸ªå·¥ä½œè€…: {', '.join(workers)}", 1, workers)
                
                # æ˜¾ç¤ºå·¥ä½œè€…å¤„ç†
                self._update_progress(ProcessingStage.WORKER_PROCESSING, "å·¥ä½œè€…å¤„ç†ä¸­", 2, workers)
            
            # 4. æ ¹æ®ç­–ç•¥æ‰§è¡Œå¤„ç†
            if operation == "create":
                result.update(self._create_new_document(content, content_type, metadata, options, strategy))
            elif operation == "update":
                result.update(self._update_existing_document(content, target_file, metadata, options, strategy))
            elif operation == "analyze":
                result.update(self._analyze_content(content, content_type, metadata, options))
            else:
                result["errors"].append(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}")
                self._update_progress(ProcessingStage.COMPLETED, "å¤„ç†å¤±è´¥", total_steps, 
                                    workers, f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}")
                return result
            
            # 5. å®Œæˆå¤„ç†
            if len(result["errors"]) == 0:
                result["success"] = True
                self._update_progress(ProcessingStage.COMPLETED, "å¤„ç†å®Œæˆ", total_steps, workers)
            else:
                self._update_progress(ProcessingStage.COMPLETED, "å¤„ç†å¤±è´¥", total_steps, 
                                    workers, "; ".join(result["errors"]))
            
            # æ·»åŠ æœ€ç»ˆè¿›åº¦ä¿¡æ¯åˆ°ç»“æœä¸­
            result["progress"] = self.current_progress.to_dict() if self.current_progress else {}
            
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            result["errors"].append(error_msg)
            
            if self.current_progress:
                self._update_progress(ProcessingStage.COMPLETED, "å¤„ç†å¼‚å¸¸", 
                                    self.current_progress.total_steps, 
                                    self.current_progress.workers, error_msg)
                result["progress"] = self.current_progress.to_dict()
            
        return result
    
    def _analyze_task_complexity(self, content: str, operation: str, options: Dict[str, Any]) -> str:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©å¤„ç†ç­–ç•¥"""
        # åŸºæœ¬å¤æ‚åº¦å› ç´ 
        factors = {
            'content_length': len(content),
            'operation_type': operation,
            'has_structure': bool(options.get('force_structure', False)),
            'requires_links': bool(options.get('enable_linking', True)),
            'batch_mode': bool(options.get('batch_mode', False))
        }
        
        # é€‰æ‹©ç­–ç•¥
        if factors['content_length'] > 10000:
            if operation == "create":
                return "hierarchical_processing"
            else:
                return "streaming_processing"
        elif factors['batch_mode']:
            return "batch_processing"
        elif operation == "update":
            return "incremental_update"
        else:
            return "standard_processing"
    
    def _create_new_document(self, content: str, content_type: str, metadata: Dict[str, Any], 
                           options: Dict[str, Any], strategy: str) -> Dict[str, Any]:
        """åˆ›å»ºæ–°æ–‡æ¡£"""
        result = {
            "success": False,
            "result": {},
            "output_file": "",
            "doc_id": "",
            "statistics": {},
            "errors": []
        }
        
        try:
            doc_id = str(uuid.uuid4())
            
            # Step 1: å†…å®¹è§£æ
            logger.info("å¼€å§‹å†…å®¹è§£æ...")
            if self.current_progress:
                complexity = self.current_progress.complexity
                if complexity != TaskComplexity.SIMPLE:
                    self._update_progress(ProcessingStage.WORKER_PROCESSING, "ğŸ¤– å†…å®¹è§£æå™¨å¤„ç†ä¸­...", 3, ["å†…å®¹è§£æå™¨"])
            
            parse_input = {
                "content": content,
                "type": content_type,
                "metadata": metadata
            }
            
            if strategy == "hierarchical_processing":
                parse_result = self._hierarchical_content_parsing(parse_input)
            elif strategy == "streaming_processing":
                parse_result = self._streaming_content_parsing(parse_input)
            else:
                parse_result = self.content_parser.process(parse_input)
            
            if not parse_result.get("parsed_content"):
                result["errors"].append("å†…å®¹è§£æå¤±è´¥")
                return result
            
            # Step 2: ç»“æ„åŒ–æ„å»º
            logger.info("å¼€å§‹ç»“æ„åŒ–æ„å»º...")
            if self.current_progress:
                complexity = self.current_progress.complexity
                if complexity == TaskComplexity.MEDIUM:
                    self._update_progress(ProcessingStage.WORKER_PROCESSING, "ğŸ—ï¸ ç»“æ„æ„å»ºå™¨å¤„ç†ä¸­...", 4, ["ç»“æ„æ„å»ºå™¨"])
                elif complexity == TaskComplexity.COMPLEX:
                    self._update_progress(ProcessingStage.WORKER_PROCESSING, "ğŸ—ï¸ ç»“æ„æ„å»ºå™¨å’ŒAIåˆ†æå™¨å¤„ç†ä¸­...", 4, ["ç»“æ„æ„å»ºå™¨", "AIåˆ†æå™¨"])
            
            structure_input = parse_result.copy()
            structure_result = self.structure_builder.process(structure_input)
            
            if not structure_result.get("structured_content"):
                result["errors"].append("ç»“æ„åŒ–æ„å»ºå¤±è´¥")
                return result
            
            # Step 3: é“¾æ¥å‘ç°
            if options.get("enable_linking", True):
                logger.info("å¼€å§‹é“¾æ¥å‘ç°...")
                if self.current_progress:
                    complexity = self.current_progress.complexity
                    if complexity == TaskComplexity.MEDIUM:
                        self._update_progress(ProcessingStage.WORKER_PROCESSING, "ğŸ”— é“¾æ¥å‘ç°å™¨å¤„ç†ä¸­...", 5, ["é“¾æ¥å‘ç°å™¨"])
                    elif complexity == TaskComplexity.COMPLEX:
                        self._update_progress(ProcessingStage.WORKER_PROCESSING, "ğŸ”— æ¦‚å¿µæå–å™¨å’Œå…³ç³»åˆ†æå™¨å¤„ç†ä¸­...", 5, ["æ¦‚å¿µæå–å™¨", "å…³ç³»åˆ†æå™¨"])
                
                link_input = {
                    "concepts": structure_result.get("concepts", []),
                    "structured_content": structure_result.get("structured_content", ""),
                    "metadata": structure_result.get("metadata", {}),
                    "doc_id": doc_id
                }
                link_result = self.link_discoverer.process(link_input)
                
                # ä½¿ç”¨æ›´æ–°åçš„å†…å®¹
                final_content = link_result.get("updated_content", structure_result["structured_content"])
            else:
                link_result = {"concept_links": [], "existing_links": [], "relationship_map": {}}
                final_content = structure_result["structured_content"]
            
            # Step 4: ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
            if options.get("enable_vector_db", True):
                logger.info("ä¿å­˜åˆ°å‘é‡æ•°æ®åº“...")
                if self.current_progress:
                    complexity = self.current_progress.complexity
                    if complexity == TaskComplexity.COMPLEX:
                        self._update_progress(ProcessingStage.FINALIZING, "ä¿å­˜åˆ°å‘é‡æ•°æ®åº“", 6)
                
                try:
                    self.vector_db.add_document(
                        content=final_content,
                        metadata={
                            **structure_result.get("metadata", {}),
                            "doc_id": doc_id,
                            "original_type": content_type,
                            "processing_strategy": strategy
                        },
                        doc_id=doc_id
                    )
                    
                    # æ·»åŠ æ¦‚å¿µ
                    concepts = structure_result.get("concepts", [])
                    if concepts:
                        self.vector_db.add_concepts(concepts, doc_id)
                        
                except Exception as e:
                    logger.warning(f"ä¿å­˜åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                    result["errors"].append(f"å‘é‡æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            
            # Step 5: ä¿å­˜æ–‡ä»¶
            if self.current_progress:
                complexity = self.current_progress.complexity
                if complexity == TaskComplexity.SIMPLE:
                    self._update_progress(ProcessingStage.FINALIZING, "ä¿å­˜æ–‡ä»¶", 2)
                else:
                    self._update_progress(ProcessingStage.FINALIZING, "ä¿å­˜æ–‡ä»¶", 
                                        self.current_progress.completed_steps + 1)
            
            output_file = self._save_to_file(final_content, structure_result, doc_id)
            
            # Step 6: æ›´æ–°é“¾æ¥æ•°æ®åº“
            if options.get("enable_linking", True):
                logger.info("æ›´æ–°é“¾æ¥æ•°æ®åº“...")
                if self.current_progress:
                    self._update_progress(ProcessingStage.FINALIZING, "æ›´æ–°é“¾æ¥æ•°æ®åº“", 
                                        self.current_progress.completed_steps + 1)
                
                try:
                    # å¤„ç†å•ä¸ªæ–‡æ¡£çš„é“¾æ¥æ›´æ–°
                    self.link_manager._process_document(output_file)
                    # é‡æ–°è§£ææ‰€æœ‰é“¾æ¥
                    self.link_manager._resolve_all_links()
                except Exception as e:
                    logger.warning(f"æ›´æ–°é“¾æ¥æ•°æ®åº“å¤±è´¥: {e}")
                    result["errors"].append(f"é“¾æ¥æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
            
            # ç»„è£…ç»“æœ
            result.update({
                "success": True,
                "result": {
                    "content": final_content,
                    "concepts": structure_result.get("concepts", []),
                    "outline": structure_result.get("outline", {}),
                    "tags": structure_result.get("tags", []),
                    "links": link_result.get("concept_links", []),
                    "external_links": link_result.get("existing_links", []),
                    "relationship_map": link_result.get("relationship_map", {})
                },
                "output_file": output_file,
                "doc_id": doc_id,
                "statistics": {
                    "original_length": len(content),
                    "processed_length": len(final_content),
                    "concept_count": len(structure_result.get("concepts", [])),
                    "internal_links": len(link_result.get("concept_links", [])),
                    "external_links": len(link_result.get("existing_links", [])),
                    "processing_strategy": strategy
                }
            })
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ–‡æ¡£å¤±è´¥: {str(e)}")
            result["errors"].append(f"åˆ›å»ºæ–‡æ¡£å¤±è´¥: {str(e)}")
        
        return result
    
    def _hierarchical_content_parsing(self, parse_input: Dict[str, Any]) -> Dict[str, Any]:
        """å±‚æ¬¡åŒ–å†…å®¹è§£æ"""
        content = parse_input["content"]
        
        # ä½¿ç”¨æ–‡æœ¬å¤„ç†å™¨è¿›è¡Œå±‚æ¬¡åŒ–å¤„ç†
        strategy = self.text_processor.choose_processing_strategy(content)
        
        if strategy == "hierarchical":
            chunks = self.text_processor.hierarchical_processing(content)
        else:
            chunks = self.text_processor.hybrid_processing(content)
        
        # åˆ†åˆ«å¤„ç†æ¯ä¸ªå—ï¼Œç„¶ååˆå¹¶
        all_parsed_content = []
        combined_metadata = {}
        
        for chunk in chunks:
            chunk_input = parse_input.copy()
            chunk_input["content"] = chunk.content
            
            chunk_result = self.content_parser.process(chunk_input)
            all_parsed_content.append(chunk_result.get("parsed_content", ""))
            
            # åˆå¹¶å…ƒæ•°æ®
            chunk_metadata = chunk_result.get("metadata", {})
            for key, value in chunk_metadata.items():
                if key not in combined_metadata:
                    combined_metadata[key] = value
                elif isinstance(value, list):
                    combined_metadata[key].extend(value)
        
        # åˆå¹¶ç»“æœ
        return {
            "parsed_content": "\n\n".join(all_parsed_content),
            "content_type": parse_input.get("type", "text"),
            "structure": {"chunks": len(chunks), "processing": "hierarchical"},
            "metadata": combined_metadata,
            "chunks": [chunk.content for chunk in chunks]
        }
    
    def _streaming_content_parsing(self, parse_input: Dict[str, Any]) -> Dict[str, Any]:
        """æµå¼å†…å®¹è§£æ"""
        content = parse_input["content"]
        
        # ä½¿ç”¨æµå¼å¤„ç†
        chunk_generator = self.text_processor.streaming_processing(content)
        
        all_parsed_content = []
        combined_metadata = {}
        chunk_count = 0
        
        for chunk in chunk_generator:
            chunk_input = parse_input.copy()
            chunk_input["content"] = chunk.content
            
            chunk_result = self.content_parser.process(chunk_input)
            all_parsed_content.append(chunk_result.get("parsed_content", ""))
            
            # åˆå¹¶å…ƒæ•°æ®
            chunk_metadata = chunk_result.get("metadata", {})
            for key, value in chunk_metadata.items():
                if key not in combined_metadata:
                    combined_metadata[key] = value
                elif isinstance(value, list):
                    combined_metadata[key].extend(value)
            
            chunk_count += 1
        
        return {
            "parsed_content": "\n\n".join(all_parsed_content),
            "content_type": parse_input.get("type", "text"),
            "structure": {"chunks": chunk_count, "processing": "streaming"},
            "metadata": combined_metadata,
            "chunks": all_parsed_content
        }
    
    def _update_existing_document(self, content: str, target_file: str, metadata: Dict[str, Any], 
                                options: Dict[str, Any], strategy: str) -> Dict[str, Any]:
        """æ›´æ–°ç°æœ‰æ–‡æ¡£"""
        result = {
            "success": False,
            "result": {},
            "output_file": "",
            "doc_id": "",
            "statistics": {},
            "errors": []
        }
        
        try:
            # 1. è¯»å–ç°æœ‰æ–‡æ¡£
            if not os.path.exists(target_file):
                result["errors"].append(f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
                return result
            
            with open(target_file, 'r', encoding='utf-8') as f:
                existing_content = f.read()
            
            # 2. æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ID (ä»æ–‡ä»¶åæˆ–å†…å®¹ä¸­æå–)
            doc_id = self._extract_doc_id(target_file, existing_content)
            
            # 3. åˆ†ææ–°å†…å®¹ä¸ç°æœ‰å†…å®¹çš„å…³ç³»
            similarity_docs = self.vector_db.search_similar_documents(content, n_results=3)
            
            # 4. å†³å®šæ›´æ–°ç­–ç•¥
            if similarity_docs and any(d['similarity'] > 0.8 for d in similarity_docs):
                # é«˜ç›¸ä¼¼åº¦ï¼Œè¿›è¡Œå¢é‡æ›´æ–°
                updated_result = self._incremental_update(content, existing_content, doc_id, metadata)
            else:
                # ä½ç›¸ä¼¼åº¦ï¼Œä½œä¸ºæ–°å†…å®¹åˆå¹¶
                updated_result = self._merge_new_content(content, existing_content, doc_id, metadata)
            
            result.update(updated_result)
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")
            result["errors"].append(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")
        
        return result
    
    def _analyze_content(self, content: str, content_type: str, metadata: Dict[str, Any], 
                        options: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ä½†ä¸åˆ›å»ºæ–‡æ¡£"""
        result = {
            "success": False,
            "result": {},
            "output_file": "",
            "doc_id": "",
            "statistics": {},
            "errors": []
        }
        
        try:
            # åªè¿›è¡Œå†…å®¹è§£æå’Œç»“æ„åŒ–åˆ†æ
            parse_input = {
                "content": content,
                "type": content_type,
                "metadata": metadata
            }
            
            parse_result = self.content_parser.process(parse_input)
            structure_result = self.structure_builder.process(parse_result)
            
            # åˆ†æç›¸å…³å†…å®¹
            related_docs = self.vector_db.search_similar_documents(content, n_results=5)
            related_concepts = self.vector_db.search_related_concepts(content, n_results=10)
            
            result.update({
                "success": True,
                "result": {
                    "analysis": {
                        "content_type": parse_result.get("content_type"),
                        "complexity": structure_result.get("metadata", {}).get("complexity_assessed"),
                        "main_concepts": structure_result.get("concepts", [])[:5],
                        "estimated_reading_time": parse_result.get("structure", {}).get("estimated_reading_time", 0)
                    },
                    "related_documents": related_docs,
                    "related_concepts": related_concepts,
                    "suggestions": {
                        "recommended_tags": structure_result.get("tags", []),
                        "potential_links": len(related_concepts),
                        "similar_documents": len(related_docs)
                    }
                },
                "statistics": {
                    "original_length": len(content),
                    "concept_count": len(structure_result.get("concepts", [])),
                    "related_doc_count": len(related_docs),
                    "related_concept_count": len(related_concepts)
                }
            })
            
        except Exception as e:
            logger.error(f"åˆ†æå†…å®¹å¤±è´¥: {str(e)}")
            result["errors"].append(f"åˆ†æå†…å®¹å¤±è´¥: {str(e)}")
        
        return result
    
    def _incremental_update(self, new_content: str, existing_content: str, doc_id: str, 
                          metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°ç°æœ‰æ–‡æ¡£"""
        # å®ç°å¢é‡æ›´æ–°é€»è¾‘
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…å¯ä»¥æ›´å¤æ‚
        
        # åœ¨ç°æœ‰å†…å®¹æœ«å°¾æ·»åŠ æ–°å†…å®¹
        separator = "\n\n---\n\n## è¡¥å……å†…å®¹\n\n"
        merged_content = existing_content + separator + new_content
        
        # é‡æ–°å¤„ç†æ•´ä¸ªæ–‡æ¡£
        return self._create_new_document(merged_content, "markdown", metadata, {}, "standard_processing")
    
    def _merge_new_content(self, new_content: str, existing_content: str, doc_id: str, 
                         metadata: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶æ–°å†…å®¹åˆ°ç°æœ‰æ–‡æ¡£"""
        # æ™ºèƒ½åˆå¹¶é€»è¾‘
        # è¿™é‡Œç®€åŒ–å®ç°
        
        # åˆ†ææ–°å†…å®¹çš„ä¸»é¢˜
        parse_result = self.content_parser.process({
            "content": new_content,
            "type": "auto",
            "metadata": metadata
        })
        
        structure_result = self.structure_builder.process(parse_result)
        main_topic = structure_result.get("outline", {}).get("title", "æ–°å¢å†…å®¹")
        
        # åœ¨é€‚å½“ä½ç½®æ’å…¥æ–°å†…å®¹
        new_section = f"\n\n## {main_topic}\n\n{new_content}"
        merged_content = existing_content + new_section
        
        return self._create_new_document(merged_content, "markdown", metadata, {}, "standard_processing")
    
    def _save_to_file(self, content: str, structure_result: Dict[str, Any], doc_id: str) -> str:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        title = structure_result.get("outline", {}).get("title", "çŸ¥è¯†ç¬”è®°")
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
        
        filename = f"{safe_title}_{doc_id[:8]}.md"
        filepath = os.path.join(self.knowledge_base_path, filename)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {filepath}")
        return filepath
    
    def _extract_doc_id(self, filepath: str, content: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æˆ–å†…å®¹ä¸­æå–æ–‡æ¡£ID"""
        # å°è¯•ä»æ–‡ä»¶åä¸­æå–
        filename = os.path.basename(filepath)
        parts = filename.split('_')
        if len(parts) > 1:
            potential_id = parts[-1].replace('.md', '')
            if len(potential_id) == 8:  # UUIDçš„å‰8ä½
                return potential_id
        
        # ç”Ÿæˆæ–°çš„ID
        return str(uuid.uuid4())
    
    def get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†æ•´ç†ç¼–æ’ä¸“å®¶ï¼Œè´Ÿè´£åè°ƒå„ä¸ªä¸“ä¸šAgentå®Œæˆå¤æ‚çš„çŸ¥è¯†æ•´ç†ä»»åŠ¡ã€‚
ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. åˆ†æä»»åŠ¡å¤æ‚åº¦å¹¶é€‰æ‹©æœ€ä¼˜å¤„ç†ç­–ç•¥
2. åè°ƒå†…å®¹è§£æã€ç»“æ„åŒ–æ„å»ºã€é“¾æ¥å‘ç°ç­‰å„ä¸ªç¯èŠ‚
3. ç¡®ä¿å¤„ç†æµç¨‹çš„è´¨é‡å’Œæ•ˆç‡
4. æä¾›è¯¦ç»†çš„å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯

è¯·å§‹ç»ˆä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ï¼Œç¡®ä¿çŸ¥è¯†æ•´ç†çš„è´¨é‡ã€‚"""