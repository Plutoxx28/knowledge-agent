"""
ç»“æ„åŒ–æ„å»ºå·¥ä½œè€… - è´Ÿè´£å°†è§£æåçš„å†…å®¹è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„çŸ¥è¯†ç¬”è®°æ ¼å¼
"""
import re
from typing import Dict, List, Any, Tuple, Set
from agents.base_agent import BaseAgent
from utils.text_processor import TextProcessor, DocumentChunk
import logging

logger = logging.getLogger(__name__)

class StructureBuilder(BaseAgent):
    """ç»“æ„åŒ–æ„å»ºå·¥ä½œè€…Agent"""
    
    def __init__(self):
        super().__init__(
            name="ç»“æ„åŒ–æ„å»ºä¸“å®¶",
            description="å°†è§£æåçš„å†…å®¹è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„çŸ¥è¯†ç¬”è®°æ ¼å¼"
        )
        self.text_processor = TextProcessor()
        
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†è§£æåçš„å†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–ç¬”è®°
        
        Args:
            input_data: {
                "parsed_content": str,      # è§£æåçš„å†…å®¹
                "content_type": str,        # å†…å®¹ç±»å‹
                "structure": dict,          # æ–‡æ¡£ç»“æ„
                "metadata": dict,           # å…ƒæ•°æ®
                "chunks": List[str]         # å†…å®¹å—
            }
        
        Returns:
            {
                "structured_content": str,  # ç»“æ„åŒ–çš„Markdownå†…å®¹
                "concepts": List[dict],     # æå–çš„æ¦‚å¿µåˆ—è¡¨
                "outline": dict,            # æ–‡æ¡£å¤§çº²
                "tags": List[str],          # ç”Ÿæˆçš„æ ‡ç­¾
                "metadata": dict            # æ›´æ–°çš„å…ƒæ•°æ®
            }
        """
        
        parsed_content = input_data.get("parsed_content", "")
        content_type = input_data.get("content_type", "text")
        structure = input_data.get("structure", {})
        metadata = input_data.get("metadata", {})
        chunks = input_data.get("chunks", [])
        
        # æå–æ ¸å¿ƒæ¦‚å¿µ
        concepts = self._extract_concepts(parsed_content, chunks)
        
        # ç”Ÿæˆæ–‡æ¡£å¤§çº²
        outline = self._generate_outline(parsed_content, structure, concepts)
        
        # æ„å»ºç»“æ„åŒ–å†…å®¹
        structured_content = self._build_structured_content(
            parsed_content, outline, concepts, content_type
        )
        
        # ç”Ÿæˆæ ‡ç­¾
        tags = self._generate_tags(concepts, metadata, content_type)
        
        # æ›´æ–°å…ƒæ•°æ®
        updated_metadata = self._update_metadata(metadata, concepts, outline)
        
        return {
            "structured_content": structured_content,
            "concepts": concepts,
            "outline": outline,
            "tags": tags,
            "metadata": updated_metadata
        }
    
    def _extract_concepts(self, content: str, chunks: List[str]) -> List[Dict[str, Any]]:
        """æå–å…³é”®æ¦‚å¿µ"""
        concepts = []
        
        # 1. åŸºäºè§„åˆ™çš„æ¦‚å¿µæå–
        rule_based_concepts = self._rule_based_concept_extraction(content)
        concepts.extend(rule_based_concepts)
        
        # 2. åŸºäºæ¨¡å¼çš„æ¦‚å¿µæå–
        pattern_based_concepts = self._pattern_based_concept_extraction(content)
        concepts.extend(pattern_based_concepts)
        
        # 3. åŸºäºé¢‘ç‡çš„æ¦‚å¿µæå–
        frequency_based_concepts = self._frequency_based_concept_extraction(content)
        concepts.extend(frequency_based_concepts)
        
        # 4. ğŸ¤– AIå¢å¼ºçš„æ¦‚å¿µæå–
        ai_concepts = self._ai_enhanced_concept_extraction(content)
        concepts.extend(ai_concepts)
        
        # å»é‡å’Œè¯„åˆ†
        concepts = self._deduplicate_and_score_concepts(concepts)
        
        return concepts
    
    def _rule_based_concept_extraction(self, content: str) -> List[Dict[str, Any]]:
        """åŸºäºè§„åˆ™çš„æ¦‚å¿µæå–"""
        concepts = []
        
        # å®šä¹‰æ¦‚å¿µæ¨¡å¼
        patterns = [
            # ç›´æ¥å®šä¹‰æ¨¡å¼
            (r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s*[æ˜¯ï¼š]\s*(.{10,100})', 'definition'),
            (r'([^\n]{2,30})\s*[ï¼š:]\s*([^\n]{10,200})', 'description'),
            
            # ä¸“ä¸šæœ¯è¯­æ¨¡å¼
            (r'\b([A-Z]{2,})\b', 'acronym'),
            (r'ã€([^ã€‘]+)ã€‘', 'term'),
            (r'`([^`]+)`', 'code_term'),
            
            # ä¸­æ–‡æ¦‚å¿µæ¨¡å¼
            (r'([ä¸€-é¾Ÿ]{2,8})[æ˜¯æŒ‡]', 'chinese_concept'),
            (r'æ‰€è°“([ä¸€-é¾Ÿ]{2,15})', 'chinese_definition'),
        ]
        
        for pattern, concept_type in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if concept_type in ['definition', 'description']:
                    term = match.group(1).strip()
                    definition = match.group(2).strip()
                else:
                    term = match.group(1).strip()
                    definition = ""
                
                if self._is_valid_concept(term):
                    concepts.append({
                        'term': term,
                        'definition': definition,
                        'type': concept_type,
                        'confidence': 0.8 if definition else 0.6,
                        'source': 'rule_based'
                    })
        
        return concepts
    
    def _pattern_based_concept_extraction(self, content: str) -> List[Dict[str, Any]]:
        """åŸºäºæ¨¡å¼çš„æ¦‚å¿µæå–"""
        concepts = []
        
        # æå–åˆ—è¡¨é¡¹ä¸­çš„æ¦‚å¿µ
        list_patterns = [
            r'[-*+]\s+\*\*([^*]+)\*\*[ï¼š:]\s*([^\n]+)',  # **æ¦‚å¿µ**: å®šä¹‰
            r'[-*+]\s+([^ï¼š:\n]{2,20})[ï¼š:]\s*([^\n]+)',  # æ¦‚å¿µ: å®šä¹‰
            r'^\d+\.\s+([^ï¼š:\n]{2,20})[ï¼š:]\s*([^\n]+)', # 1. æ¦‚å¿µ: å®šä¹‰
        ]
        
        for pattern in list_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                term = match.group(1).strip()
                definition = match.group(2).strip()
                
                if self._is_valid_concept(term):
                    concepts.append({
                        'term': term,
                        'definition': definition,
                        'type': 'list_item',
                        'confidence': 0.7,
                        'source': 'pattern_based'
                    })
        
        # æå–æ ‡é¢˜ä¸­çš„æ¦‚å¿µ
        heading_pattern = r'^#{1,6}\s+(.+)$'
        matches = re.finditer(heading_pattern, content, re.MULTILINE)
        for match in matches:
            title = match.group(1).strip()
            # æ¸…ç†æ ‡é¢˜ä¸­çš„æ ‡è®°ç¬¦å·
            clean_title = re.sub(r'[ã€ã€‘\[\]()ï¼ˆï¼‰]', '', title)
            
            if self._is_valid_concept(clean_title):
                concepts.append({
                    'term': clean_title,
                    'definition': '',
                    'type': 'heading',
                    'confidence': 0.6,
                    'source': 'pattern_based'
                })
        
        return concepts
    
    def _frequency_based_concept_extraction(self, content: str) -> List[Dict[str, Any]]:
        """åŸºäºé¢‘ç‡çš„æ¦‚å¿µæå–"""
        concepts = []
        
        # æå–å€™é€‰è¯æ±‡
        # ä¸­æ–‡è¯æ±‡
        chinese_terms = re.findall(r'[ä¸€-é¾Ÿ]{2,8}', content)
        # è‹±æ–‡è¯æ±‡
        english_terms = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)
        # æ··åˆè¯æ±‡
        mixed_terms = re.findall(r'[A-Za-z]+[ä¸€-é¾Ÿ]+|[ä¸€-é¾Ÿ]+[A-Za-z]+', content)
        
        all_terms = chinese_terms + english_terms + mixed_terms
        
        # ç»Ÿè®¡é¢‘ç‡
        term_freq = {}
        for term in all_terms:
            term = term.strip()
            if self._is_valid_concept(term):
                term_freq[term] = term_freq.get(term, 0) + 1
        
        # é€‰æ‹©é«˜é¢‘è¯æ±‡ä½œä¸ºæ¦‚å¿µ
        min_freq = max(2, len(content) // 2000)  # åŠ¨æ€è°ƒæ•´æœ€å°é¢‘ç‡
        for term, freq in term_freq.items():
            if freq >= min_freq:
                confidence = min(0.9, 0.3 + freq * 0.1)
                concepts.append({
                    'term': term,
                    'definition': '',
                    'type': 'high_frequency',
                    'confidence': confidence,
                    'frequency': freq,
                    'source': 'frequency_based'
                })
        
        return concepts
    
    def _is_valid_concept(self, term: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ¦‚å¿µ"""
        term = term.strip()
        
        # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
        if len(term) < 2 or len(term) > 50:
            return False
        
        # æ’é™¤å¸¸è§åœç”¨è¯
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'å°±', 'éƒ½', 'è¦', 'å¯ä»¥', 'æ²¡æœ‰',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™', 'æ€æ ·', 'å¤šå°‘'
        }
        
        if term.lower() in stop_words:
            return False
        
        # æ’é™¤çº¯æ•°å­—ã€æ ‡ç‚¹ç¬¦å·
        if term.isdigit() or not re.search(r'[a-zA-Zä¸€-é¾Ÿ]', term):
            return False
        
        # æ’é™¤è¿‡äºé€šç”¨çš„è¯æ±‡
        generic_terms = {
            'æ–¹æ³•', 'ç³»ç»Ÿ', 'é—®é¢˜', 'å†…å®¹', 'ä¿¡æ¯', 'æ•°æ®', 'ç»“æœ', 'è¿‡ç¨‹', 'åŠŸèƒ½', 'æŠ€æœ¯',
            'method', 'system', 'problem', 'content', 'information', 'data', 'result', 'process'
        }
        
        if term.lower() in generic_terms:
            return False
        
        return True
    
    def _ai_enhanced_concept_extraction(self, content: str) -> List[Dict[str, Any]]:
        """ğŸ¤– AIå¢å¼ºçš„æ¦‚å¿µæå–"""
        concepts = []
        
        try:
            logger.info("å¼€å§‹AIå¢å¼ºæ¦‚å¿µæå–...")
            
            # æ„å»ºæç¤ºè¯
            messages = [
                {
                    "role": "system",
                    "content": self.get_system_prompt()
                },
                {
                    "role": "user", 
                    "content": f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼Œæå–å…¶ä¸­çš„æ ¸å¿ƒæ¦‚å¿µå’Œé‡è¦æœ¯è¯­ï¼š

{content[:2000]}  # é™åˆ¶å†…å®¹é•¿åº¦

è¯·ä»¥JSONæ ¼å¼è¿”å›æ¦‚å¿µåˆ—è¡¨ï¼Œæ¯ä¸ªæ¦‚å¿µåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- term: æ¦‚å¿µåç§°
- definition: æ¦‚å¿µå®šä¹‰æˆ–è§£é‡Šï¼ˆå¦‚æœå†…å®¹ä¸­åŒ…å«ï¼‰
- type: æ¦‚å¿µç±»å‹ï¼ˆå¦‚ï¼štechnical_term, concept, methodologyç­‰ï¼‰
- confidence: ç½®ä¿¡åº¦(0-1)

ç¤ºä¾‹ï¼š
[
  {{"term": "æœºå™¨å­¦ä¹ ", "definition": "è®©è®¡ç®—æœºç³»ç»Ÿè‡ªåŠ¨æ”¹è¿›æ€§èƒ½çš„æ–¹æ³•", "type": "technical_term", "confidence": 0.9}},
  {{"term": "ç¥ç»ç½‘ç»œ", "definition": "", "type": "technical_term", "confidence": 0.8}}
]

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
                }
            ]
            
            # è°ƒç”¨AIæ¨¡å‹
            response = self.call_llm(messages, max_tokens=1000)
            
            if response:
                logger.info(f"AIæ¦‚å¿µæå–å“åº”: {response[:200]}...")
                
                # è§£æAIå“åº”
                try:
                    import json
                    ai_concepts_data = json.loads(response)
                    
                    for concept_data in ai_concepts_data:
                        if isinstance(concept_data, dict) and 'term' in concept_data:
                            concept = {
                                'term': concept_data.get('term', '').strip(),
                                'definition': concept_data.get('definition', '').strip(),
                                'type': concept_data.get('type', 'ai_extracted'),
                                'confidence': float(concept_data.get('confidence', 0.7)),
                                'source': 'ai_enhanced'
                            }
                            
                            # éªŒè¯æ¦‚å¿µæœ‰æ•ˆæ€§
                            if self._is_valid_concept(concept['term']):
                                concepts.append(concept)
                                
                    logger.info(f"AIæˆåŠŸæå–äº† {len(concepts)} ä¸ªæ¦‚å¿µ")
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
                    # å°è¯•ç®€å•æ–‡æœ¬è§£æ
                    concepts.extend(self._parse_ai_response_fallback(response))
                    
        except Exception as e:
            logger.error(f"AIæ¦‚å¿µæå–å¤±è´¥: {e}")
            
        return concepts
    
    def _parse_ai_response_fallback(self, response: str) -> List[Dict[str, Any]]:
        """AIå“åº”çš„å¤‡ç”¨è§£ææ–¹æ³•"""
        concepts = []
        
        # å°è¯•æå–ç±»ä¼¼æ¦‚å¿µçš„æ¨¡å¼
        patterns = [
            r'"term":\s*"([^"]+)"',
            r'æ¦‚å¿µ[:ï¼š]\s*([^\n,ï¼Œ]{2,20})',
            r'æœ¯è¯­[:ï¼š]\s*([^\n,ï¼Œ]{2,20})',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response)
            for match in matches:
                term = match.strip()
                if self._is_valid_concept(term):
                    concepts.append({
                        'term': term,
                        'definition': '',
                        'type': 'ai_extracted_fallback',
                        'confidence': 0.6,
                        'source': 'ai_enhanced_fallback'
                    })
        
        return concepts
    
    def _deduplicate_and_score_concepts(self, concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡å’Œè¯„åˆ†æ¦‚å¿µ"""
        # æŒ‰è¯æ±‡åˆå¹¶ç›¸ä¼¼æ¦‚å¿µ
        concept_groups = {}
        
        for concept in concepts:
            term = concept['term']
            key = term.lower().strip()
            
            if key not in concept_groups:
                concept_groups[key] = []
            concept_groups[key].append(concept)
        
        # åˆå¹¶åŒç»„æ¦‚å¿µ
        merged_concepts = []
        for group in concept_groups.values():
            # é€‰æ‹©æœ€å¥½çš„æ¦‚å¿µ
            best_concept = max(group, key=lambda x: x['confidence'])
            
            # åˆå¹¶å®šä¹‰
            definitions = [c['definition'] for c in group if c['definition']]
            if definitions:
                best_concept['definition'] = max(definitions, key=len)
            
            # åˆå¹¶ç±»å‹
            types = list(set(c['type'] for c in group))
            best_concept['types'] = types
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = best_concept['confidence']
            if len(definitions) > 1:
                final_score += 0.1  # å¤šå®šä¹‰åŠ åˆ†
            if len(types) > 1:
                final_score += 0.1  # å¤šç±»å‹åŠ åˆ†
                
            best_concept['final_score'] = min(1.0, final_score)
            merged_concepts.append(best_concept)
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰20ä¸ª
        merged_concepts.sort(key=lambda x: x['final_score'], reverse=True)
        return merged_concepts[:20]
    
    def _generate_outline(self, content: str, structure: Dict[str, Any], concepts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡£å¤§çº²"""
        outline = {
            "title": "",
            "sections": [],
            "main_concepts": [],
            "complexity": "medium"
        }
        
        # æå–æ ‡é¢˜
        title = self._extract_main_title(content)
        outline["title"] = title
        
        # åŸºäºç°æœ‰ç»“æ„ç”Ÿæˆç« èŠ‚
        if structure.get("headings"):
            outline["sections"] = self._structure_to_sections(structure["headings"], content)
        else:
            # åŸºäºå†…å®¹ç”Ÿæˆç« èŠ‚
            outline["sections"] = self._generate_sections_from_content(content, concepts)
        
        # é€‰æ‹©ä¸»è¦æ¦‚å¿µ
        main_concepts = [c for c in concepts if c['final_score'] > 0.7][:8]
        outline["main_concepts"] = [c['term'] for c in main_concepts]
        
        # è¯„ä¼°å¤æ‚åº¦
        outline["complexity"] = self._assess_complexity(content, concepts, outline["sections"])
        
        return outline
    
    def _extract_main_title(self, content: str) -> str:
        """æå–ä¸»æ ‡é¢˜"""
        lines = content.split('\n')
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ ‡é¢˜
        for line in lines[:10]:  # åªæŸ¥çœ‹å‰10è¡Œ
            line = line.strip()
            
            # Markdownä¸€çº§æ ‡é¢˜
            if line.startswith('# '):
                return line[2:].strip()
            
            # å…¶ä»–å¯èƒ½çš„æ ‡é¢˜æ ¼å¼
            if len(line) > 5 and len(line) < 100:
                if (line.isupper() or 
                    line.endswith('ï¼š') or 
                    line.endswith(':') or
                    re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼.]', line)):
                    return line.strip('ï¼š:')
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªéç©ºè¡Œ
        for line in lines[:5]:
            line = line.strip()
            if line and len(line) > 3:
                return line[:50]  # é™åˆ¶é•¿åº¦
        
        return "çŸ¥è¯†ç¬”è®°"
    
    def _structure_to_sections(self, headings: List[Dict[str, Any]], content: str) -> List[Dict[str, Any]]:
        """å°†ç°æœ‰ç»“æ„è½¬æ¢ä¸ºç« èŠ‚"""
        sections = []
        
        for heading in headings:
            if heading['level'] <= 3:  # åªå¤„ç†å‰ä¸‰çº§æ ‡é¢˜
                sections.append({
                    'title': heading['title'],
                    'level': heading['level'],
                    'type': 'heading_based'
                })
        
        return sections
    
    def _generate_sections_from_content(self, content: str, concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»å†…å®¹ç”Ÿæˆç« èŠ‚"""
        sections = []
        
        # åŸºäºæ¦‚å¿µå¯†åº¦åˆ†å‰²
        high_score_concepts = [c for c in concepts if c['final_score'] > 0.6]
        
        if len(high_score_concepts) >= 3:
            # ç”ŸæˆåŸºäºæ¦‚å¿µçš„ç« èŠ‚
            sections.append({'title': 'æ ¸å¿ƒæ¦‚å¿µ', 'level': 2, 'type': 'concept_based'})
            sections.append({'title': 'è¯¦ç»†è¯´æ˜', 'level': 2, 'type': 'content_based'})
            sections.append({'title': 'ç›¸å…³é“¾æ¥', 'level': 2, 'type': 'link_based'})
        else:
            # ç”Ÿæˆé€šç”¨ç« èŠ‚
            sections.append({'title': 'ä¸»è¦å†…å®¹', 'level': 2, 'type': 'content_based'})
        
        return sections
    
    def _assess_complexity(self, content: str, concepts: List[Dict[str, Any]], sections: List[Dict[str, Any]]) -> str:
        """è¯„ä¼°å†…å®¹å¤æ‚åº¦"""
        factors = {
            'length': len(content),
            'concepts': len(concepts),
            'sections': len(sections),
            'technical_terms': len([c for c in concepts if c['type'] in ['acronym', 'code_term']])
        }
        
        score = 0
        if factors['length'] > 5000: score += 1
        if factors['concepts'] > 10: score += 1
        if factors['sections'] > 5: score += 1
        if factors['technical_terms'] > 3: score += 1
        
        if score >= 3:
            return "advanced"
        elif score >= 1:
            return "intermediate"
        else:
            return "beginner"
    
    def _build_structured_content(self, content: str, outline: Dict[str, Any], 
                                concepts: List[Dict[str, Any]], content_type: str) -> str:
        """æ„å»ºç»“æ„åŒ–å†…å®¹"""
        
        # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©æ¨¡æ¿
        if content_type == "conversation":
            return self._build_conversation_format(content, outline, concepts)
        elif content_type == "url":
            return self._build_article_format(content, outline, concepts)
        else:
            return self._build_general_format(content, outline, concepts)
    
    def _build_conversation_format(self, content: str, outline: Dict[str, Any], 
                                 concepts: List[Dict[str, Any]]) -> str:
        """æ„å»ºå¯¹è¯æ ¼å¼çš„ç»“æ„åŒ–å†…å®¹"""
        sections = []
        
        # æ ‡é¢˜
        title = outline.get('title', 'å¯¹è¯è®°å½•æ•´ç†')
        sections.append(f"# {title}")
        sections.append("")
        
        # å…ƒæ•°æ®
        sections.append("## å¯¹è¯ä¿¡æ¯")
        sections.append(f"- **ç±»å‹**: å¯¹è¯è®°å½•")
        sections.append(f"- **å¤æ‚åº¦**: {outline.get('complexity', 'medium')}")
        sections.append(f"- **ä¸»è¦æ¦‚å¿µæ•°**: {len(concepts)}")
        sections.append("")
        
        # æ ¸å¿ƒæ¦‚å¿µ
        if concepts:
            sections.append("## æ ¸å¿ƒæ¦‚å¿µ")
            sections.append("")
            for concept in concepts[:8]:
                term = concept['term']
                definition = concept.get('definition', '')
                if definition:
                    sections.append(f"- **[[{term}]]**: {definition}")
                else:
                    sections.append(f"- **[[{term}]]**")
            sections.append("")
        
        # å¯¹è¯å†…å®¹
        sections.append("## å¯¹è¯å†…å®¹")
        sections.append("")
        
        # å¤„ç†å¯¹è¯å†…å®¹ï¼Œæ·»åŠ æ¦‚å¿µé“¾æ¥
        processed_content = self._add_concept_links(content, concepts)
        sections.append(processed_content)
        sections.append("")
        
        # çŸ¥è¯†å…³è”
        if len(concepts) > 3:
            sections.append("## çŸ¥è¯†é“¾")
            sections.append("")
            concept_terms = [f"[[{c['term']}]]" for c in concepts[:6]]
            sections.append(" â†’ ".join(concept_terms))
            sections.append("")
        
        return "\n".join(sections)
    
    def _build_article_format(self, content: str, outline: Dict[str, Any], 
                            concepts: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ–‡ç« æ ¼å¼çš„ç»“æ„åŒ–å†…å®¹"""
        sections = []
        
        # æ ‡é¢˜
        title = outline.get('title', 'æ–‡ç« æ•´ç†')
        sections.append(f"# {title}")
        sections.append("")
        
        # å…ƒæ•°æ®
        sections.append("## æ–‡ç« ä¿¡æ¯")
        sections.append(f"- **ç±»å‹**: æ–‡ç« æ•´ç†")
        sections.append(f"- **å¤æ‚åº¦**: {outline.get('complexity', 'medium')}")
        sections.append(f"- **ä¸»è¦æ¦‚å¿µ**: {len(concepts)}ä¸ª")
        sections.append("")
        
        # æ ¸å¿ƒæ¦‚å¿µå®šä¹‰
        if concepts:
            sections.append("## æ ¸å¿ƒæ¦‚å¿µ")
            sections.append("")
            for concept in concepts[:10]:
                term = concept['term']
                definition = concept.get('definition', '')
                if definition and len(definition) > 10:
                    sections.append(f"### [[{term}]]")
                    sections.append(definition)
                    sections.append("")
                else:
                    sections.append(f"- **[[{term}]]**")
            sections.append("")
        
        # ä¸»è¦å†…å®¹
        sections.append("## ä¸»è¦å†…å®¹")
        sections.append("")
        
        # æ·»åŠ æ¦‚å¿µé“¾æ¥å¹¶åˆ†æ®µ
        processed_content = self._add_concept_links(content, concepts)
        formatted_content = self._format_content_sections(processed_content, outline['sections'])
        sections.append(formatted_content)
        sections.append("")
        
        # çŸ¥è¯†å…³è”
        if len(concepts) > 2:
            sections.append("## çŸ¥è¯†é“¾æ¥")
            sections.append("")
            
            # ä¸»è¦æ¦‚å¿µé“¾
            main_concepts = [c['term'] for c in concepts[:5] if c['final_score'] > 0.7]
            if main_concepts:
                sections.append("### ä¸»è¦æ¦‚å¿µé“¾")
                concept_links = [f"[[{term}]]" for term in main_concepts]
                sections.append(" â†’ ".join(concept_links))
                sections.append("")
            
            # ç›¸å…³æ¦‚å¿µ
            related_concepts = [c['term'] for c in concepts[5:10]]
            if related_concepts:
                sections.append("### ç›¸å…³æ¦‚å¿µ")
                for term in related_concepts:
                    sections.append(f"- [[{term}]]")
                sections.append("")
        
        return "\n".join(sections)
    
    def _build_general_format(self, content: str, outline: Dict[str, Any], 
                            concepts: List[Dict[str, Any]]) -> str:
        """æ„å»ºé€šç”¨æ ¼å¼çš„ç»“æ„åŒ–å†…å®¹"""
        sections = []
        
        # æ ‡é¢˜
        title = outline.get('title', 'çŸ¥è¯†æ•´ç†')
        sections.append(f"# {title}")
        sections.append("")
        
        # æ¦‚å¿µå®šä¹‰
        if concepts:
            sections.append("## æ ¸å¿ƒæ¦‚å¿µ")
            sections.append("")
            for concept in concepts[:8]:
                term = concept['term']
                definition = concept.get('definition', '')
                if definition:
                    sections.append(f"- **[[{term}]]**: {definition}")
                else:
                    sections.append(f"- **[[{term}]]**")
            sections.append("")
        
        # ä¸»è¦å†…å®¹
        sections.append("## è¯¦ç»†å†…å®¹")
        sections.append("")
        processed_content = self._add_concept_links(content, concepts)
        sections.append(processed_content)
        sections.append("")
        
        return "\n".join(sections)
    
    def _add_concept_links(self, content: str, concepts: List[Dict[str, Any]]) -> str:
        """åœ¨å†…å®¹ä¸­æ·»åŠ æ¦‚å¿µé“¾æ¥"""
        processed_content = content
        
        # æŒ‰é•¿åº¦æ’åºï¼Œå…ˆå¤„ç†é•¿æ¦‚å¿µé¿å…è¢«çŸ­æ¦‚å¿µè¦†ç›–
        sorted_concepts = sorted(concepts, key=lambda x: len(x['term']), reverse=True)
        
        for concept in sorted_concepts[:15]:  # é™åˆ¶å¤„ç†æ•°é‡
            term = concept['term']
            
            # é¿å…åœ¨å·²æœ‰é“¾æ¥ä¸­æ·»åŠ é“¾æ¥
            if f"[[{term}]]" not in processed_content:
                # ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                pattern = r'\b' + re.escape(term) + r'\b'
                replacement = f"[[{term}]]"
                
                # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œé¿å…è¿‡åº¦é“¾æ¥
                processed_content = re.sub(pattern, replacement, processed_content, count=1)
        
        return processed_content
    
    def _format_content_sections(self, content: str, sections: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å†…å®¹ç« èŠ‚"""
        if not sections:
            return content
        
        # å¦‚æœå·²æœ‰æ˜ç¡®ç« èŠ‚ç»“æ„ï¼Œä¿æŒåŸæ ·
        if any('heading_based' == s.get('type') for s in sections):
            return content
        
        # å¦åˆ™ï¼Œå°è¯•æ™ºèƒ½åˆ†æ®µ
        paragraphs = content.split('\n\n')
        if len(paragraphs) <= 3:
            return content
        
        # æŒ‰æ®µè½åˆ†ç»„
        formatted_sections = []
        paragraphs_per_section = max(1, len(paragraphs) // len(sections))
        
        for i, section in enumerate(sections):
            start_idx = i * paragraphs_per_section
            end_idx = start_idx + paragraphs_per_section
            if i == len(sections) - 1:  # æœ€åä¸€ä¸ªsectionåŒ…å«å‰©ä½™æ‰€æœ‰æ®µè½
                end_idx = len(paragraphs)
            
            section_content = '\n\n'.join(paragraphs[start_idx:end_idx])
            if section_content.strip():
                formatted_sections.append(f"### {section['title']}")
                formatted_sections.append("")
                formatted_sections.append(section_content)
                formatted_sections.append("")
        
        return '\n'.join(formatted_sections)
    
    def _generate_tags(self, concepts: List[Dict[str, Any]], metadata: Dict[str, Any], content_type: str) -> List[str]:
        """ç”Ÿæˆæ ‡ç­¾"""
        tags = set()
        
        # åŸºäºå†…å®¹ç±»å‹çš„æ ‡ç­¾
        type_tags = {
            'conversation': ['#å¯¹è¯', '#AIé—®ç­”'],
            'url': ['#æ–‡ç« ', '#å¤–éƒ¨é“¾æ¥'],
            'markdown': ['#æ–‡æ¡£', '#æ•´ç†'],
            'text': ['#æ–‡æœ¬', '#ç¬”è®°']
        }
        
        tags.update(type_tags.get(content_type, ['#çŸ¥è¯†']))
        
        # åŸºäºå¤æ‚åº¦çš„æ ‡ç­¾
        complexity = metadata.get('difficulty', 'intermediate')
        complexity_tags = {
            'beginner': '#åˆçº§',
            'intermediate': '#ä¸­çº§', 
            'advanced': '#é«˜çº§'
        }
        tags.add(complexity_tags.get(complexity, '#ä¸­çº§'))
        
        # åŸºäºæ¦‚å¿µçš„æ ‡ç­¾
        technical_concepts = [c for c in concepts if c['type'] in ['acronym', 'code_term']]
        if technical_concepts:
            tags.add('#æŠ€æœ¯')
        
        ai_related_terms = ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'LLM', 'GPT', 'RAG']
        if any(term in str(concepts) for term in ai_related_terms):
            tags.add('#AI')
        
        # åŸºäºä¸»é¢˜çš„æ ‡ç­¾ï¼ˆä»metadataçš„topicsä¸­è·å–ï¼‰
        topics = metadata.get('topics', [])
        for topic in topics[:3]:  # æœ€å¤š3ä¸ªä¸»é¢˜æ ‡ç­¾
            if topic and len(topic) < 10:
                tags.add(f'#{topic}')
        
        return sorted(list(tags))
    
    def _update_metadata(self, metadata: Dict[str, Any], concepts: List[Dict[str, Any]], 
                        outline: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°å…ƒæ•°æ®"""
        updated_metadata = metadata.copy()
        
        updated_metadata.update({
            'processed_time': None,  # å®é™…å®ç°æ—¶æ·»åŠ æ—¶é—´æˆ³
            'concept_count': len(concepts),
            'main_concepts': [c['term'] for c in concepts[:5]],
            'outline_sections': len(outline.get('sections', [])),
            'complexity_assessed': outline.get('complexity', 'medium'),
            'processing_version': '1.0'
        })
        
        return updated_metadata